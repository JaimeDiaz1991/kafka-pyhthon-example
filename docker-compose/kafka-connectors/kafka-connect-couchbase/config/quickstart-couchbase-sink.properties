# Arbitrary unique name for the connector. Attempting to register
# two connectors with the same name will fail.
name=test-couchbase-sink

# The Java class for the connector.
connector.class=com.couchbase.connect.kafka.CouchbaseSinkConnector

# The maximum number of tasks that should be created for this connector.
tasks.max=2

# Read from these Kafka topics (comma-separated list).
topics=couchbase-sink-example

# Connect to this Couchbase cluster (comma-separated list of bootstrap nodes).
connection.cluster_address=127.0.0.1
connection.timeout.ms=2000

# Optionally connect to Couchbase Server over a secure channel.
#   connection.ssl.enabled=true
#   connection.ssl.keystore.location=/tmp/keystore
#   connection.ssl.keystore.password=secret

# Write to this Couchbase bucket using these credentials.
connection.bucket=receiver
connection.username=receiver
connection.password=secret

# Optionally set the Couchbase document ID from a field of the message body
# (specified using JSON Pointer notation). Optionally remove this field from
# the document before writing to Couchbase.
#   couchbase.document.id=/id
#   couchbase.remove.document.id=true

# The following *.converter properties are appropriate when reading from
# a topic whose messages have String (or null) keys and raw JSON values,
# without schemas. These are the correct settings to use with the code in
# the examples/json-producer directory.
key.converter=org.apache.kafka.connect.storage.StringConverter
value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=false

# If you're using Confluent, the Couchbase Sink Connector can also read
# messages published in the Avro format. Replace the above *.converter
# properties with the following (modifying the schema registry URL if needed):
#   key.converter=io.confluent.connect.avro.AvroConverter
#   key.converter.schema.registry.url=http://localhost:8081
#   value.converter=io.confluent.connect.avro.AvroConverter
#   value.converter.schema.registry.url=http://localhost:8081
